---
title: "Text mining"
output: html_notebook
---
### Preliminary step: importing the data

```{r}
df_influenster <- read.csv("influenster_iphoneX.csv", sep = ",")

df_influenster <- df_influenster[complete.cases(df_influenster), ]

df_influenster <- as.data.frame(df_influenster)
```
Our dataframe is ready

### First step: creating a corpus

```{r}
library(tm)
myCorpus <- Corpus(VectorSource(df_influenster$CommentBox_Content))
```

### Second step: ponctuation filter, etc

```{r}
myCorpus <- tm_map(myCorpus, tolower)
myCorpus <- tm_map(myCorpus, removePunctuation)
myCorpus <- tm_map(myCorpus, removeNumbers)
```

```{r}
toSpace <- content_transformer(function(x, pattern) gsub(pattern, " ", x))

myCorpus <- tm_map(myCorpus, toSpace, "/|@|\\|")
myCorpus <- tm_map(myCorpus, toSpace, "\n")
```

### Third step: remove white space

```{r}
myCorpus <- tm_map(myCorpus, stripWhitespace)
```

### Fourth step: remove stop words

```{r}
myCorpus <- tm_map(myCorpus, removeWords, stopwords("english"))
```


### Fifth step: steaming or lemmatization

Don't know which algorithm I should use.
Moreover, the meaning of the word may be erased. 

### Sixth step: n-grams

To analyze the textual data, we use a Document-Term Matrix (DTM) representation: documents as the rows, terms/words as the columns, frequency of the term in the document as the entries. Because the number of unique words in the corpus the dimension can be large.
```{r}
review_dtm <- DocumentTermMatrix(myCorpus)
review_dtm
inspect(review_dtm[50:55, 50:55])
```

To reduce the dimension of the DTM, we can emove the less frequent terms such that the sparsity is less than 0.95

```{r}
review_dtm = removeSparseTerms(review_dtm, 0.99)
review_dtm
inspect(review_dtm[1,1:10])
```

Let's draw a word cloud:

```{r}
findFreqTerms(review_dtm, lowfreq = 7)
```


```{r}
library(wordcloud)
freq = data.frame(sort(colSums(as.matrix(review_dtm)), decreasing=TRUE))
wordcloud(rownames(freq), freq[,1], max.words=50, colors=brewer.pal(1, "Dark2"))
```




