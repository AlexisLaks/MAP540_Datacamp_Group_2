{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text-mining on webscraped phone reviews (iPhone + Samsung)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Group members:\n",
    "\n",
    "Allesandro Girelli\n",
    "\n",
    "Cyprien Nielly\n",
    "\n",
    "Katie Chang\n",
    "\n",
    "Sebastien Moeller\n",
    "\n",
    "Viktor Malesevic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction:\n",
    "\n",
    "The aim of this project is to do text-mining and analysis on phone reviews from different webpages (Amazon, Reddit, Influenster, etc...) in order to provide advice to phone manufacturers on potential issues faced by customers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Webscraping the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We decided to webscrape Amazon reviews, which we did on R using the 'rvest' package.\n",
    "\n",
    "The results is found in the csv file 'Reviews.csv'. This file concatenates data from iPhone X, iPhone 8, and Samsung S8 reviews."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Pre-processing the webscraped data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Importing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Before reading the data we import some usefull libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Pandas for data manipulation\n",
    "import pandas as pd\n",
    "\n",
    "# nltk for all text data pre-processing\n",
    "\n",
    "#nltk.download()\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk import pos_tag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTA**: the nltk package needs to be download thanks to the command 'nltk.download()' which then opens a window. On this window click on 'all packages' and then 'download'. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we import the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('Reviews.csv', encoding = 'ISO-8859-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>source</th>\n",
       "      <th>product</th>\n",
       "      <th>comments</th>\n",
       "      <th>stars</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Amazon</td>\n",
       "      <td>Samsung S8</td>\n",
       "      <td>BEWARE!99% of the negative reviews are SELLER ...</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Amazon</td>\n",
       "      <td>Samsung S8</td>\n",
       "      <td>So far the best phone I ever had. Man it's be...</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Amazon</td>\n",
       "      <td>Samsung S8</td>\n",
       "      <td>I was skeptical about buying this phone off Am...</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Amazon</td>\n",
       "      <td>Samsung S8</td>\n",
       "      <td>This phone should be a no-brainer. Easily the ...</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Amazon</td>\n",
       "      <td>Samsung S8</td>\n",
       "      <td>I haven't owned a Galaxy phone since the Galax...</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  source     product  \\\n",
       "0           1  Amazon  Samsung S8   \n",
       "1           2  Amazon  Samsung S8   \n",
       "2           3  Amazon  Samsung S8   \n",
       "3           4  Amazon  Samsung S8   \n",
       "4           5  Amazon  Samsung S8   \n",
       "\n",
       "                                            comments  stars  \n",
       "0  BEWARE!99% of the negative reviews are SELLER ...    5.0  \n",
       "1   So far the best phone I ever had. Man it's be...    5.0  \n",
       "2  I was skeptical about buying this phone off Am...    5.0  \n",
       "3  This phone should be a no-brainer. Easily the ...    3.0  \n",
       "4  I haven't owned a Galaxy phone since the Galax...    4.0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del data['Unnamed: 0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>product</th>\n",
       "      <th>comments</th>\n",
       "      <th>stars</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Amazon</td>\n",
       "      <td>Samsung S8</td>\n",
       "      <td>BEWARE!99% of the negative reviews are SELLER ...</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Amazon</td>\n",
       "      <td>Samsung S8</td>\n",
       "      <td>So far the best phone I ever had. Man it's be...</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Amazon</td>\n",
       "      <td>Samsung S8</td>\n",
       "      <td>I was skeptical about buying this phone off Am...</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Amazon</td>\n",
       "      <td>Samsung S8</td>\n",
       "      <td>This phone should be a no-brainer. Easily the ...</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Amazon</td>\n",
       "      <td>Samsung S8</td>\n",
       "      <td>I haven't owned a Galaxy phone since the Galax...</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   source     product                                           comments  \\\n",
       "0  Amazon  Samsung S8  BEWARE!99% of the negative reviews are SELLER ...   \n",
       "1  Amazon  Samsung S8   So far the best phone I ever had. Man it's be...   \n",
       "2  Amazon  Samsung S8  I was skeptical about buying this phone off Am...   \n",
       "3  Amazon  Samsung S8  This phone should be a no-brainer. Easily the ...   \n",
       "4  Amazon  Samsung S8  I haven't owned a Galaxy phone since the Galax...   \n",
       "\n",
       "   stars  \n",
       "0    5.0  \n",
       "1    5.0  \n",
       "2    5.0  \n",
       "3    3.0  \n",
       "4    4.0  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stars</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>5746.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>4.320745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.247721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             stars\n",
       "count  5746.000000\n",
       "mean      4.320745\n",
       "std       1.247721\n",
       "min       1.000000\n",
       "25%       4.000000\n",
       "50%       5.000000\n",
       "75%       5.000000\n",
       "max       5.000000"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### So far we have a dataset of 5700 reviews, composed of 4 columns:\n",
    "\n",
    "The source: Amazon\n",
    "\n",
    "The product: iPhone 8, iPhone X or Samsung S8\n",
    "\n",
    "The comment/review: raw text written by the customer\n",
    "\n",
    "The rating: from 1 to 5 stars."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For the moment we will only focus on the comments for each phone model (without rating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "commentsiX = data[data['product'] == 'iPhone X']\n",
    "commentsi8 = data[data['product'] == 'iPhone 8']\n",
    "commentsS8 = data[data['product'] == 'Samsung S8']\n",
    "\n",
    "commentsiX = commentsiX['comments']\n",
    "commentsi8 = commentsi8['comments']\n",
    "commentsS8 = commentsS8['comments']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Removing unnecessary characters and words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we first need to do is to remove special characters, accents, punctuation, and put all words in lower case.\n",
    "\n",
    "For code see 2.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Tokenizing comments into monograms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After that, we need to 'tokenize' the comments into 'monograms', 'bigrams' or 'Ngrams'...\n",
    "We choose to build 'monograms': this is basically separating the words one by one for each comment (for bigrams it is two by two, etc...)\n",
    "\n",
    "\n",
    "For code see 2.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Lemmatizing the monograms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we create three functions referring to each other in order to do 'special characters removal', 'tokenizing' and 'lemmatizing'. This is all done in the function 'tokenList' which uses the two functions 'get_wordnet_pos' and 'lemmatize'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**get_wordnet_pos** is the function that is useful for lemmatizing:  it assigns to each words its status (adjective, noun, verb, etc...). It uses the function **wordnet** from nltk.\n",
    "\n",
    "**lemmatize** is the function lemmatizing each word, using the function **get_wordnet_pos** we just created, the function **pos_tag** and the function **WordLemmatizer.lemmatize** from nltk.\n",
    "\n",
    "Finally **tokenList** is the main function:\n",
    "\n",
    "It first removes the special characters from comments, makes sure each variable is a string, and deletes some particular words like 'iphone', 'samsung', etc...\n",
    "\n",
    "Then, it tokenizes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Given a list of tokens passed through nltk.pos_tag(tokens), this returns the\n",
    "# pos argument needed for the lemmitization in a new list of pairs: (word, type)\n",
    "def get_wordnet_pos(tokensTag):\n",
    "    # tokensTag is a list of pairs of tuples and cannot be modified\n",
    "    tokenNew = []  \n",
    "    for i in range(len(tokensTag)):\n",
    "        # Save the current word being identified\n",
    "        tokenNew.append([tokensTag[i][0]])\n",
    "        # Append the type\n",
    "        if tokensTag[i][1][0] == 'J':\n",
    "            tokenNew[i].append(wordnet.ADJ)\n",
    "            \n",
    "        elif tokensTag[i][1][0] == 'V':\n",
    "            tokenNew[i].append(wordnet.VERB)\n",
    "            \n",
    "        elif tokensTag[i][1][0] == 'N':\n",
    "            tokenNew[i].append(wordnet.NOUN)\n",
    "            \n",
    "        elif tokensTag[i][1][0] == 'R':\n",
    "            tokenNew[i].append(wordnet.ADV)\n",
    "        \n",
    "        elif tokensTag[i][1][0] == '?':\n",
    "            tokenNew[i].append(wordnet.ADJ_SAT)\n",
    "        else:\n",
    "            tokenNew[i].append(wordnet.ADJ)\n",
    "            \n",
    "    return tokenNew\n",
    "\n",
    "# Smart lemmitization !\n",
    "def lemmatize(words):\n",
    "        \n",
    "    wordType = get_wordnet_pos(pos_tag(words))\n",
    "    wordnet_lemmatizer = WordNetLemmatizer()    \n",
    "    for i in range(len(wordType)):\n",
    "        words[i] = wordnet_lemmatizer.lemmatize(wordType[i][0], pos = wordType[i][1])\n",
    "    \n",
    "    return words \n",
    "\n",
    "def tokenList(my_list):\n",
    "    # Lowercase all characters \n",
    "    # (like this the same word will contribute to the same token count)\n",
    "    comments = [item.lower() for item in my_list]\n",
    "    # We establish a dictionary of the transformation: characters to replace with a space\n",
    "    # We also want to remove context words that don't have meaning\n",
    "    transformation = {a:' ' for a in ['@','/','#','.','\\\\','!',',','(',')','{','}','[',']','-','~', '*','?','+', '8', '7', '6', ';', ':', '|']}\n",
    "    transB = {b:'' for b in ['','’','\"']}\n",
    "    comments = [item.translate(str.maketrans(transformation)) for item in comments]\n",
    "    comments = [item.translate(str.maketrans(transB)) for item in comments]\n",
    "    comments = [item.replace('iphone', ' ').replace('samsung', ' ').replace('galaxy', ' ').replace('apple', ' ').replace('plus', ' ').replace(\n",
    "            ' x ', ' ').replace('’', '').replace(\"'\", '').replace('http', '').replace('https', '').replace('com', ' ').replace('co', ' ').replace(\n",
    "                    '201', ' ').replace('0', ' ').replace('â\\x80\\x99', '').replace('í¢ä\\x89åä\\x8b¢', '').replace('phone', ' ') for item in comments]\n",
    "    \n",
    "    # nltk's tokenizer\n",
    "    tkzer = TweetTokenizer(preserve_case = False, strip_handles = True, reduce_len = True)\n",
    "    tokens = [tkzer.tokenize(item) for item in comments]\n",
    "    \n",
    "    tokens = []\n",
    "    #wordnet_lemmatizer = WordNetLemmatizer()\n",
    "    for idx in range(len(comments)):\n",
    "        # Remove engish stopwords\n",
    "        words = ([word for word in comments[idx].split() if word not in stopwords.words('english')])\n",
    "        \n",
    "        words = lemmatize(words)\n",
    "        #for idy in range(len(words)):\n",
    "            # Lemmatize each word\n",
    "            \n",
    "            #words[idy] = wordnet_lemmatizer.lemmatize(words[idy], pos = 'v')\n",
    "        \n",
    "        tokens.append(words)\n",
    "        # Progress report\n",
    "        print('Tokenizing: ',idx+1, ' / ', len(comments))\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some other useful functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# given a list of ordered tokens from a document, the function will return-\n",
    "# a list of neighbouring groups of size n.\n",
    "def nGrams(input_list, n):\n",
    "    return list(zip(*[input_list[i:] for i in range(n)]))\n",
    "\n",
    "# Returns nGrams from a corpus of tokens\n",
    "def listGrams(input_list, n):\n",
    "    output =[]\n",
    "    for idx in range(len(input_list)):\n",
    "        temp = nGrams(input_list[idx], n)\n",
    "        output = output + temp\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Creation of a Term Frequency & Inverse Term Frequency matrix (TF-IDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#%% TF - IDF Matrix Construction\n",
    "# All lemmitized tokens joined by comment\n",
    "lemmiX = []\n",
    "lemmi8 = []\n",
    "lemmS8 = []\n",
    "\n",
    "for idx in range(len(tokensiX)):\n",
    "    lemmiX.append(' '.join(tokensiX[idx]))\n",
    "\n",
    "for idx in range(len(tokensi8)):\n",
    "    lemmi8.append(' '.join(tokensi8[idx]))\n",
    "\n",
    "for idx in range(len(tokensS8)):\n",
    "    lemmS8.append(' '.join(tokensS8[idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#%%\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=2, stop_words='english')\n",
    "termdoc = tfidf_vectorizer.fit_transform(lemmiX)\n",
    "TFM = pd.DataFrame(termdoc.todense()).replace(0, '')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Non-negative Matrix Factorization (NMF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#%%\n",
    "n_dimensions = 40 # This can also be interpreted as topics in this case. This is the \"beauty\" of NMF. 10 is arbitrary\n",
    "model = NMF(n_components=40, init='random')\n",
    "W = model.fit_transform(termdoc) \n",
    "H = model.components_ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#%%\n",
    "W = pd.DataFrame(W).replace(0, '')\n",
    "H = pd.DataFrame(H).replace(0, '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#%%\n",
    "# Since NMF dimensions can be interpreted as topics, let's look at the dimensions\n",
    "words = tfidf_vectorizer.get_feature_names()\n",
    "n_top_words = 20 # print 10 words by dimension. You can change this number\n",
    "\n",
    "# Loop for each dimension: what words are the most dominant in each dimension\n",
    "for i_dimension, dimension in enumerate(model.components_):\n",
    "    print(\"Topic #%d:\" % i_dimension)\n",
    "    print(\" \".join([words[i] for i in dimension.argsort()[:-n_top_words - 1:-1]]))\n",
    "print()\n",
    "\n",
    "# Can you interpret these dimensions as humanly intelligible topics?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Topic extraction with Latent Dirichlet Allocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#%% LDA ANALYSIS\n",
    "from gensim import corpora, models\n",
    "\n",
    "#dictionary = corpora.Dictionary(tokens)\n",
    "dictionaryiX = corpora.Dictionary(tokensiX)\n",
    "dictionaryi8 = corpora.Dictionary(tokensi8)\n",
    "dictionaryS8 = corpora.Dictionary(tokensS8)\n",
    "#print(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#%%\n",
    "#corpus = [dictionary.doc2bow(text) for text in tokens]\n",
    "corpusiX = [dictionaryiX.doc2bow(text) for text in tokensiX]\n",
    "corpusi8 = [dictionaryi8.doc2bow(text) for text in tokensi8]\n",
    "corpusS8 = [dictionaryS8.doc2bow(text) for text in tokensS8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#%%\n",
    "# Long computation time!!!\n",
    "#ldamodel = models.ldamodel.LdaModel(corpus, num_topics = 40, id2word = dictionary, passes = 10)\n",
    "ldaiX = models.ldamodel.LdaModel(corpusiX, num_topics = 40, id2word = dictionaryiX, passes = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#%%\n",
    "ldai8 = models.ldamodel.LdaModel(corpusi8, num_topics = 40, id2word = dictionaryi8, passes = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#%%\n",
    "ldaS8 = models.ldamodel.LdaModel(corpusS8, num_topics = 40, id2word = dictionaryS8, passes = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#%%\n",
    "# Top 10 words associated with the 40 topics we clustered the data into\n",
    "#print(ldamodel.print_topics(num_topics = 40, num_words = 10))\n",
    "print(ldaiX.print_topics(num_topics = 40, num_words = 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#%%\n",
    "print(ldai8.print_topics(num_topics = 40, num_words = 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#%%\n",
    "print(ldaS8.print_topics(num_topics = 40, num_words = 10))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
