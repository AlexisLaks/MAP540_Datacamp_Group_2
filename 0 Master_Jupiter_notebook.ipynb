{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text-mining on webscraped phone reviews (iPhone + Samsung)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Group members:\n",
    "\n",
    "Allesandro Girelli\n",
    "\n",
    "Cyprien Nielly\n",
    "\n",
    "Katie Chang\n",
    "\n",
    "Sebastien Moeller\n",
    "\n",
    "Viktor Malesevic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction:\n",
    "\n",
    "The aim of this project is to do text-mining and analysis on phone reviews from different webpages (Amazon, Reddit, Influenster, etc...) in order to provide advice to phone manufacturers on potential issues faced by customers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Webscraping the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We decided to webscrape Amazon reviews, which we did on R using the 'rvest' package. The second set of reviews we scraped were Google Shopping reviews with the code explained below.\n",
    "\n",
    "The results is found in our csv file, 'Reviews.csv'. This file concatenates data from iPhone X, iPhone 8, and Samsung S8 reviews.\n",
    "\n",
    "###### Scrapping Google Shopping Reviews:\n",
    "To scrap the Google Shopping reviews we used the BeautifulSoup package, as well as url to html downloader urllib.request."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import urllib.request as urll\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The way the code is built is that we are given the url of the first page of reviews and the functions scrap all pages available for the product.\n",
    "\n",
    "The following function takes a url and scraps all reviews from one page only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def googlePageScrap(url):\n",
    "    # Initializing BeautifulSoup\n",
    "    page = urll.urlopen(url)\n",
    "    soup = BeautifulSoup(page, 'html.parser')\n",
    "    \n",
    "    # Saving the nodes containing the reviews and ratings\n",
    "    rev = soup.find_all('div', attrs={'class':'review-content'})\n",
    "    rat = soup.find_all('div', attrs={'class':'_OBj'})\n",
    "    \n",
    "    # Ratings need to be extracted from the soup\n",
    "    rating = []\n",
    "    for i in range(len(rat)):\n",
    "        # Convert to string otherwise .find() doesn't work\n",
    "        rat[i] = str(rat[i])\n",
    "        # Finding the index of the rating\n",
    "        idx = rat[i].find('aria-label=\"')+len('aria-label=\"')\n",
    "        # Save the rating in the rating list\n",
    "        rating.append(int(rat[i][idx]))\n",
    "    \n",
    "    # The first entry is the average rating between all, therefore we delete it\n",
    "    rating = rating[1:]\n",
    "    \n",
    "    # Building the output\n",
    "    output = []\n",
    "    for i in range(len(rev)):\n",
    "        # Building meta data\n",
    "        meta = []\n",
    "        meta.append(rating[i])\n",
    "        # Reviews can use the function .text to extract the actual reviews\n",
    "        meta.append(rev[i].text[1:-20])\n",
    "        # Save meta data with the comment ( Rating + Review )\n",
    "        output.append(meta)\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To be able to go to the next page, we need to know how many pages there are. The `googleMaxPage(url)` function takes a url of a page of reviews as an input and returns the total number of pages of reviews. This is because only 10 reviews are displayed per page and we want as many reviews as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Returns the number of pages of reviews that need to be visited\n",
    "def googleMaxPage(url):\n",
    "    page = urll.urlopen(url)\n",
    "    soup = BeautifulSoup(page, 'html.parser')\n",
    "    maxPage = soup.find('span', attrs={'class':'pag-n-to-n-txt'})\n",
    "    maxPage = re.sub('[^0-9]','', maxPage.text[7:])\n",
    "    # This returns the total number of reviews\n",
    "    maxPage = int(maxPage)\n",
    "    # There are 10 reviews per page\n",
    "    maxPage = int(maxPage/10)-1\n",
    "    \n",
    "    return maxPage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, using the following function we are able to visit each page of reviews and scrap the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Scraps all reviews starting from page 1 as the input url\n",
    "def googleScrap(url):\n",
    "    \n",
    "    maxPage = googleMaxPage(url)\n",
    "    output = googlePageScrap(url)\n",
    "    \n",
    "    # Go to each page of reviews and add them to the output list\n",
    "    for i in range(maxPage):\n",
    "        urlPage = str(url + ',rstart:'+ str(i+1) +'0')\n",
    "        new = googlePageScrap(urlPage)\n",
    "        output = output + new\n",
    "        print(i+1,' / ',maxPage)\n",
    "    \n",
    "    # Convert the list into a pandas dataframe\n",
    "    output_df = pd.DataFrame(output)\n",
    "    output_df.columns = ['stars', 'comments']\n",
    "\n",
    "    return output_df "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The actual scrapping was done on the following URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "url = 'https://www.google.com/shopping/product/8330308525491645368/reviews?output=search&q=apple+iphone+8&prds=paur:ClkAsKraX0HAK8DTxuQ7a_QLy1VVmmdGjqus04Pco-mYuwDAnhwY-2yRVwjDEGi_xEsNVx11gFrfhnTT-NU5F1Cv8xkFN2t2KRFYe2S0bJHgJvnYxGmE7Xr8KxIZAFPVH73QWHZfxejgLJJNz1emAuExXPnAxg,rsort:1'\n",
    "iPhone8 = googleScrap(url)\n",
    "\n",
    "url = 'https://www.google.com/shopping/product/5196767965601398683/reviews?output=search&q=iphone+x&oq=iphone+x&prds=paur:ClkAsKraX6xXlTCTDvTg5n66BfqjZtUzj5mRPstz9QYmLjncZZBAQRRtobM8Pe5XLEZX0CP8x5UxXIzT52WhOhO2moZSRoKU0aTE6QE0f-R3zq1xhh45Jvza8BIZAFPVH70FzB4_QX4D05ZaAMc8F9sjUFRwvg,rsort:1'\n",
    "iPhoneX = googleScrap(url)\n",
    "\n",
    "url = 'https://www.google.com/shopping/product/2874873357294577697/reviews?output=search&q=galaxy+s8&oq=galaxy+s8&prds=paur:ClkAsKraX4MdXEv-XobV-tsudUmMvrTaF0oUQFrnUCBf-gngBeSUnGe1TQzRN-qEvUxg11H4haqP6POwtI-P9rAtftKbUh-e4yFNzeeFNldak82GgWHBlGI__xIZAFPVH72dPmpO1V1eoP7Y9BbJQh6EoOBh5Q,rsort:1'\n",
    "samsungS8 = googleScrap(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reviews were exported to csv files to be combined with the Amazon reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "iPhoneX.to_csv('GoogleiPhoneX.csv')\n",
    "\n",
    "iPhone8.to_csv('GoogleiPhone8.csv')\n",
    "\n",
    "samsungS8.to_csv('GoogleSamsungS8.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merging the data\n",
    "\n",
    "Now that we have all our reviews we combine them into one csv file to be used in part 2.\n",
    "\n",
    "We begin by importing all csv files, merg the data to follow the same format, combine everything into a data frame and then export the information to `Reviews.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data0 = pd.read_csv('AmazonSamsungS8.csv', index_col = 0)\n",
    "data1 = pd.read_csv('AmazoniPhone8.csv', index_col = 0)\n",
    "data2 = pd.read_csv('AmazoniPhoneX.csv', index_col = 0)\n",
    "data3 = pd.read_csv('GoogleSamsungS8.csv', index_col = 0)\n",
    "data4 = pd.read_csv('GoogleiPhone8.csv', index_col = 0)\n",
    "data5 = pd.read_csv('GoogleiPhoneX.csv', index_col = 0)\n",
    "\n",
    "# Unifying format to merge meta data\n",
    "data0 = pd.DataFrame(data0[['comments', 'stars']])\n",
    "data0['source'] = 'Amazon'\n",
    "data0['product'] = 'Samsung S8'\n",
    "data0 = data0[['source', 'product', 'comments', 'stars']]\n",
    "\n",
    "data1 = pd.DataFrame(data1[['comments', 'stars']])\n",
    "data1['source'] = 'Amazon'\n",
    "data1['product'] = 'iPhone 8'\n",
    "data1 = data1[['source', 'product', 'comments', 'stars']]\n",
    "\n",
    "data2 = pd.DataFrame(data2[['comments', 'stars']])\n",
    "data2['source'] = 'Amazon'\n",
    "data2['product'] = 'iPhone X'\n",
    "data2 = data1[['source', 'product', 'comments', 'stars']]\n",
    "\n",
    "data3 = pd.DataFrame(data3[['comments', 'stars']])\n",
    "data3['source'] = 'Google Shopping'\n",
    "data3['product'] = 'Samsung S8'\n",
    "data3 = data3[['source', 'product', 'comments', 'stars']]\n",
    "\n",
    "data4 = pd.DataFrame(data4[['comments', 'stars']])\n",
    "data4['source'] = 'Google Shopping'\n",
    "data4['product'] = 'iPhone 8'\n",
    "data4 = data4[['source', 'product', 'comments', 'stars']]\n",
    "\n",
    "data5 = pd.DataFrame(data5[['comments', 'stars']])\n",
    "data5['source'] = 'Google Shopping'\n",
    "data5['product'] = 'iPhone X'\n",
    "data5 = data5[['source', 'product', 'comments', 'stars']]\n",
    "\n",
    "data6 = pd.DataFrame(data6[['CommentBox_Content', 'CommentBox_Rating']])\n",
    "data6.columns = ['comments', 'stars']\n",
    "data6['source'] = 'Influenster'\n",
    "data6['product'] = 'iPhone X'\n",
    "data6 = data6[['source', 'product', 'comments', 'stars']]\n",
    "\n",
    "group2Meta = pd.concat([data0, data1, data2, data3, data4, data5, data6])\n",
    "\n",
    "group2Meta.to_csv('Reviews.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Pre-processing the webscraped data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Importing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Before reading the data we import some usefull libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Pandas for data manipulation\n",
    "import pandas as pd\n",
    "\n",
    "# nltk for all text data pre-processing\n",
    "\n",
    "#nltk.download()\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk import pos_tag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTA**: the nltk package needs to be download thanks to the command '`nltk.download()`' which then opens a window. On this window click on 'all packages' and then 'download'. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we import the dataset:\n",
    "\n",
    "As the reviews come from different sources and contain characters such as emojis we need to specify the encoding option as the default can no longer interpret the raw data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('Reviews.csv', encoding = 'ISO-8859-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>source</th>\n",
       "      <th>product</th>\n",
       "      <th>comments</th>\n",
       "      <th>stars</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Amazon</td>\n",
       "      <td>Samsung S8</td>\n",
       "      <td>BEWARE!99% of the negative reviews are SELLER ...</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Amazon</td>\n",
       "      <td>Samsung S8</td>\n",
       "      <td>So far the best phone I ever had. Man it's be...</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Amazon</td>\n",
       "      <td>Samsung S8</td>\n",
       "      <td>I was skeptical about buying this phone off Am...</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Amazon</td>\n",
       "      <td>Samsung S8</td>\n",
       "      <td>This phone should be a no-brainer. Easily the ...</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Amazon</td>\n",
       "      <td>Samsung S8</td>\n",
       "      <td>I haven't owned a Galaxy phone since the Galax...</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  source     product  \\\n",
       "0           1  Amazon  Samsung S8   \n",
       "1           2  Amazon  Samsung S8   \n",
       "2           3  Amazon  Samsung S8   \n",
       "3           4  Amazon  Samsung S8   \n",
       "4           5  Amazon  Samsung S8   \n",
       "\n",
       "                                            comments  stars  \n",
       "0  BEWARE!99% of the negative reviews are SELLER ...    5.0  \n",
       "1   So far the best phone I ever had. Man it's be...    5.0  \n",
       "2  I was skeptical about buying this phone off Am...    5.0  \n",
       "3  This phone should be a no-brainer. Easily the ...    3.0  \n",
       "4  I haven't owned a Galaxy phone since the Galax...    4.0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del data['Unnamed: 0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>product</th>\n",
       "      <th>comments</th>\n",
       "      <th>stars</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Amazon</td>\n",
       "      <td>Samsung S8</td>\n",
       "      <td>BEWARE!99% of the negative reviews are SELLER ...</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Amazon</td>\n",
       "      <td>Samsung S8</td>\n",
       "      <td>So far the best phone I ever had. Man it's be...</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Amazon</td>\n",
       "      <td>Samsung S8</td>\n",
       "      <td>I was skeptical about buying this phone off Am...</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Amazon</td>\n",
       "      <td>Samsung S8</td>\n",
       "      <td>This phone should be a no-brainer. Easily the ...</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Amazon</td>\n",
       "      <td>Samsung S8</td>\n",
       "      <td>I haven't owned a Galaxy phone since the Galax...</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   source     product                                           comments  \\\n",
       "0  Amazon  Samsung S8  BEWARE!99% of the negative reviews are SELLER ...   \n",
       "1  Amazon  Samsung S8   So far the best phone I ever had. Man it's be...   \n",
       "2  Amazon  Samsung S8  I was skeptical about buying this phone off Am...   \n",
       "3  Amazon  Samsung S8  This phone should be a no-brainer. Easily the ...   \n",
       "4  Amazon  Samsung S8  I haven't owned a Galaxy phone since the Galax...   \n",
       "\n",
       "   stars  \n",
       "0    5.0  \n",
       "1    5.0  \n",
       "2    5.0  \n",
       "3    3.0  \n",
       "4    4.0  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stars</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>5746.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>4.320745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.247721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             stars\n",
       "count  5746.000000\n",
       "mean      4.320745\n",
       "std       1.247721\n",
       "min       1.000000\n",
       "25%       4.000000\n",
       "50%       5.000000\n",
       "75%       5.000000\n",
       "max       5.000000"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### So far we have a dataset of 5700 reviews, composed of 4 columns:\n",
    "\n",
    "The source: Amazon\n",
    "\n",
    "The product: iPhone 8, iPhone X or Samsung S8\n",
    "\n",
    "The comment/review: raw text written by the customer\n",
    "\n",
    "The rating: from 1 to 5 stars."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For the moment we will only focus on the comments for each phone model (without rating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "commentsiX = data[data['product'] == 'iPhone X']\n",
    "commentsi8 = data[data['product'] == 'iPhone 8']\n",
    "commentsS8 = data[data['product'] == 'Samsung S8']\n",
    "\n",
    "commentsiX = commentsiX['comments']\n",
    "commentsi8 = commentsi8['comments']\n",
    "commentsS8 = commentsS8['comments']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Removing unnecessary characters and words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we first need to do is to remove special characters, accents, punctuation, and put all words in lower case.\n",
    "\n",
    "For code see 2.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Tokenizing comments into monograms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After that, we need to 'tokenize' the comments into 'monograms', 'bigrams' or 'Ngrams'...\n",
    "We choose to build 'monograms': this is basically separating the words one by one for each comment (for bigrams it is two by two, etc...)\n",
    "\n",
    "\n",
    "For code see 2.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Lemmatizing the monograms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we create three functions referring to each other in order to do 'special characters removal', 'tokenizing' and 'lemmatizing'. This is all done in the function 'tokenList' which uses the two functions 'get_wordnet_pos' and 'lemmatize'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**get_wordnet_pos** is the function that is useful for lemmatizing:  it assigns to each words its status (adjective, noun, verb, etc...). It uses the function **wordnet** from nltk.\n",
    "\n",
    "**lemmatize** is the function lemmatizing each word, using the function **get_wordnet_pos** we just created, the function **pos_tag** and the function **WordLemmatizer.lemmatize** from nltk.\n",
    "\n",
    "Finally **tokenList** is the main function:\n",
    "\n",
    "It first removes the special characters from comments, makes sure each variable is a string, and deletes some particular words like 'iphone', 'samsung', etc...\n",
    "\n",
    "Then, it tokenizes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Given a list of tokens passed through nltk.pos_tag(tokens), this returns the\n",
    "# pos argument needed for the lemmitization in a new list of pairs: (word, type)\n",
    "def get_wordnet_pos(tokensTag):\n",
    "    # tokensTag is a list of pairs of tuples and cannot be modified\n",
    "    tokenNew = []  \n",
    "    for i in range(len(tokensTag)):\n",
    "        # Save the current word being identified\n",
    "        tokenNew.append([tokensTag[i][0]])\n",
    "        # Append the type\n",
    "        if tokensTag[i][1][0] == 'J':\n",
    "            tokenNew[i].append(wordnet.ADJ)\n",
    "            \n",
    "        elif tokensTag[i][1][0] == 'V':\n",
    "            tokenNew[i].append(wordnet.VERB)\n",
    "            \n",
    "        elif tokensTag[i][1][0] == 'N':\n",
    "            tokenNew[i].append(wordnet.NOUN)\n",
    "            \n",
    "        elif tokensTag[i][1][0] == 'R':\n",
    "            tokenNew[i].append(wordnet.ADV)\n",
    "        \n",
    "        elif tokensTag[i][1][0] == '?':\n",
    "            tokenNew[i].append(wordnet.ADJ_SAT)\n",
    "        else:\n",
    "            tokenNew[i].append(wordnet.ADJ)\n",
    "            \n",
    "    return tokenNew\n",
    "\n",
    "# Smart lemmitization !\n",
    "def lemmatize(words):\n",
    "        \n",
    "    wordType = get_wordnet_pos(pos_tag(words))\n",
    "    wordnet_lemmatizer = WordNetLemmatizer()    \n",
    "    for i in range(len(wordType)):\n",
    "        words[i] = wordnet_lemmatizer.lemmatize(wordType[i][0], pos = wordType[i][1])\n",
    "    \n",
    "    return words \n",
    "\n",
    "def tokenList(my_list):\n",
    "    # Lowercase all characters \n",
    "    # (like this the same word will contribute to the same token count)\n",
    "    comments = [item.lower() for item in my_list]\n",
    "    # We establish a dictionary of the transformation: characters to replace with a space\n",
    "    # We also want to remove context words that don't have meaning\n",
    "    transformation = {a:' ' for a in ['@','/','#','.','\\\\','!',',','(',')','{','}','[',']','-','~', '*','?','+', '8', '7', '6', ';', ':', '|']}\n",
    "    transB = {b:'' for b in ['','’','\"']}\n",
    "    comments = [item.translate(str.maketrans(transformation)) for item in comments]\n",
    "    comments = [item.translate(str.maketrans(transB)) for item in comments]\n",
    "    comments = [item.replace('iphone', ' ').replace('samsung', ' ').replace('galaxy', ' ').replace('apple', ' ').replace('plus', ' ').replace(\n",
    "            ' x ', ' ').replace('’', '').replace(\"'\", '').replace('http', '').replace('https', '').replace('com', ' ').replace('co', ' ').replace(\n",
    "                    '201', ' ').replace('0', ' ').replace('â\\x80\\x99', '').replace('í¢ä\\x89åä\\x8b¢', '').replace('phone', ' ') for item in comments]\n",
    "    \n",
    "    # nltk's tokenizer\n",
    "    tkzer = TweetTokenizer(preserve_case = False, strip_handles = True, reduce_len = True)\n",
    "    tokens = [tkzer.tokenize(item) for item in comments]\n",
    "    \n",
    "    tokens = []\n",
    "    #wordnet_lemmatizer = WordNetLemmatizer()\n",
    "    for idx in range(len(comments)):\n",
    "        # Remove engish stopwords\n",
    "        words = ([word for word in comments[idx].split() if word not in stopwords.words('english')])\n",
    "        \n",
    "        words = lemmatize(words)\n",
    "        #for idy in range(len(words)):\n",
    "            # Lemmatize each word\n",
    "            \n",
    "            #words[idy] = wordnet_lemmatizer.lemmatize(words[idy], pos = 'v')\n",
    "        \n",
    "        tokens.append(words)\n",
    "        # Progress report\n",
    "        print('Tokenizing: ',idx+1, ' / ', len(comments))\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some other useful functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# given a list of ordered tokens from a document, the function will return-\n",
    "# a list of neighbouring groups of size n.\n",
    "def nGrams(input_list, n):\n",
    "    return list(zip(*[input_list[i:] for i in range(n)]))\n",
    "\n",
    "# Returns nGrams from a corpus of tokens\n",
    "def listGrams(input_list, n):\n",
    "    output =[]\n",
    "    for idx in range(len(input_list)):\n",
    "        temp = nGrams(input_list[idx], n)\n",
    "        output = output + temp\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Creation of a Term Frequency & Inverse Term Frequency matrix (TF-IDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#%% TF - IDF Matrix Construction\n",
    "# All lemmitized tokens joined by comment\n",
    "lemmiX = []\n",
    "lemmi8 = []\n",
    "lemmS8 = []\n",
    "\n",
    "for idx in range(len(tokensiX)):\n",
    "    lemmiX.append(' '.join(tokensiX[idx]))\n",
    "\n",
    "for idx in range(len(tokensi8)):\n",
    "    lemmi8.append(' '.join(tokensi8[idx]))\n",
    "\n",
    "for idx in range(len(tokensS8)):\n",
    "    lemmS8.append(' '.join(tokensS8[idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#%%\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=2, stop_words='english')\n",
    "termdoc = tfidf_vectorizer.fit_transform(lemmiX)\n",
    "TFM = pd.DataFrame(termdoc.todense()).replace(0, '')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Non-negative Matrix Factorization (NMF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#%%\n",
    "n_dimensions = 40 # This can also be interpreted as topics in this case. This is the \"beauty\" of NMF. 10 is arbitrary\n",
    "model = NMF(n_components=40, init='random')\n",
    "W = model.fit_transform(termdoc) \n",
    "H = model.components_ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#%%\n",
    "W = pd.DataFrame(W).replace(0, '')\n",
    "H = pd.DataFrame(H).replace(0, '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#%%\n",
    "# Since NMF dimensions can be interpreted as topics, let's look at the dimensions\n",
    "words = tfidf_vectorizer.get_feature_names()\n",
    "n_top_words = 20 # print 10 words by dimension. You can change this number\n",
    "\n",
    "# Loop for each dimension: what words are the most dominant in each dimension\n",
    "for i_dimension, dimension in enumerate(model.components_):\n",
    "    print(\"Topic #%d:\" % i_dimension)\n",
    "    print(\" \".join([words[i] for i in dimension.argsort()[:-n_top_words - 1:-1]]))\n",
    "print()\n",
    "\n",
    "# Can you interpret these dimensions as humanly intelligible topics?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Topic extraction with Latent Dirichlet Allocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#%% LDA ANALYSIS\n",
    "from gensim import corpora, models\n",
    "\n",
    "#dictionary = corpora.Dictionary(tokens)\n",
    "dictionaryiX = corpora.Dictionary(tokensiX)\n",
    "dictionaryi8 = corpora.Dictionary(tokensi8)\n",
    "dictionaryS8 = corpora.Dictionary(tokensS8)\n",
    "#print(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#%%\n",
    "#corpus = [dictionary.doc2bow(text) for text in tokens]\n",
    "corpusiX = [dictionaryiX.doc2bow(text) for text in tokensiX]\n",
    "corpusi8 = [dictionaryi8.doc2bow(text) for text in tokensi8]\n",
    "corpusS8 = [dictionaryS8.doc2bow(text) for text in tokensS8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#%%\n",
    "# Long computation time!!!\n",
    "#ldamodel = models.ldamodel.LdaModel(corpus, num_topics = 40, id2word = dictionary, passes = 10)\n",
    "ldaiX = models.ldamodel.LdaModel(corpusiX, num_topics = 40, id2word = dictionaryiX, passes = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#%%\n",
    "ldai8 = models.ldamodel.LdaModel(corpusi8, num_topics = 40, id2word = dictionaryi8, passes = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#%%\n",
    "ldaS8 = models.ldamodel.LdaModel(corpusS8, num_topics = 40, id2word = dictionaryS8, passes = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#%%\n",
    "# Top 10 words associated with the 40 topics we clustered the data into\n",
    "#print(ldamodel.print_topics(num_topics = 40, num_words = 10))\n",
    "print(ldaiX.print_topics(num_topics = 40, num_words = 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#%%\n",
    "print(ldai8.print_topics(num_topics = 40, num_words = 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#%%\n",
    "print(ldaS8.print_topics(num_topics = 40, num_words = 10))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
