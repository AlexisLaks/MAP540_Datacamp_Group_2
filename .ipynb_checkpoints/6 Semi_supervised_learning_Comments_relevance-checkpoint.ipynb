{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semi supervised learning \n",
    "\n",
    "#### aim of this notebook : build a classifer for relevant / not relevant (that is classify a comment as a review related to a default, issue)\n",
    "### first build a classifier in supervised approach using labeled data \n",
    "### second build a classifer based on labeled data + unlabeled data to which we propagated labels \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm, tqdm_notebook # progress bars in Jupyter\n",
    "#import newspaper # download newspapers' data easily\n",
    "from time import time # measure the computation time of a python code\n",
    "import pandas as pd # the most basic & powerful data manipulation tool\n",
    "import numpy as np # Here, mostly used for np.nan\n",
    "import langdetect # detect the language of text\n",
    "import stop_words # handles stop words in many languages without having to rebuild them everytime\n",
    "import spacy # NLP library for POS tagging\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "import re\n",
    "import itertools\n",
    "# For spacy use \"pip install spacy\", then \"python -m spacy download en\" to download English text mining modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tqdm.pandas()\n",
    "#tqdm_notebook()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('labeled_data.csv', engine='python') # label data only -> used for supervised model \n",
    "\n",
    "dfu = pd.read_csv('data_unlabeled.csv', encoding = 'utf-8') \n",
    "# unlabeled data -> used to together with lable data for semi supervised learning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10997, 15)\n",
      "                                               text  apps_update  \\\n",
      "0  dope video brian iâve been subscribe for years          0.0   \n",
      "\n",
      "   battery_life_charging  battery_overheat  camera  connectivity  \\\n",
      "0                    0.0               0.0     0.0           0.0   \n",
      "\n",
      "   customerservice  locking_system  memory_storage  screen  software_bugs  \\\n",
      "0              0.0             0.0             0.0     0.0            0.0   \n",
      "\n",
      "   sound  system  water_damage  issue  \n",
      "0    0.0     0.0           0.0    0.0  \n"
     ]
    }
   ],
   "source": [
    "print(df.shape)\n",
    "print(df.head(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "apps_update                82\n",
       "battery_life_charging     147\n",
       "battery_overheat           22\n",
       "camera                    100\n",
       "connectivity              109\n",
       "customerservice           109\n",
       "locking_system            206\n",
       "memory_storage            156\n",
       "screen                    316\n",
       "software_bugs             140\n",
       "sound                      43\n",
       "system                    343\n",
       "water_damage                6\n",
       "issue                    1498\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[[c for c in df.columns if c not in ['text', 'tokens']]].sum().map(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create features\n",
    "\n",
    "### one important thing to have in mind when building a model : to make feature engineering separately on train and test. If you don't do that, you will incoporate info from the test set into the train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.models import Phrases\n",
    "from gensim import corpora\n",
    "import stop_words\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import NMF\n",
    "\n",
    "#nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Function to clean and process the reviews\n",
    "def cleaning_data(df) :\n",
    "    STOPWORDS = stop_words.get_stop_words(language='en')\n",
    "    #df.drop_duplicates(inplace= True) # Drop duplicated sentences\n",
    "    df = df[~df['text'].isnull()] # Remove empty sentences\n",
    "        \n",
    "    # Remove special characters and punctucation\n",
    "    df['clean_review']= [ re.sub('[^A-Za-z]+',' ', e ) for e in df['text'].apply(lambda x : x.lower())]\n",
    "    \n",
    "    # Remove empty clean_review\n",
    "    df = df[~df['clean_review'].isnull()]\n",
    "    df = df[~(df['clean_review']==' ')]\n",
    "    df.reset_index(inplace=True, drop=True) # Reset index\n",
    "    \n",
    "    df['tokens'] = df['clean_review'].map(word_tokenize)\n",
    "    df['nb_tokens'] = df['tokens'].map(len)\n",
    "    \n",
    "    ## keep only sentences with at least 3 tokens\n",
    "    df = df[df['nb_tokens']>2]\n",
    "    \n",
    "    # remove stopwords\n",
    "    df['tokens'] = df['tokens'].apply(lambda x: [i for i in x if i not in STOPWORDS])\n",
    "\n",
    "    stemmer = SnowballStemmer(\"english\")\n",
    "    df['stemmed_text'] = df[\"tokens\"].apply(lambda x: [stemmer.stem(y) for y in x])\n",
    "    df['joined_stemmed_text'] = [' '.join(word for word in word_list) for word_list in df.stemmed_text ]\n",
    "\n",
    "    return df\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0    9499\n",
       "1.0    1498\n",
       "Name: issue, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.issue.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## split between train and test at the beginning \n",
    "# we will use the same test set for supervised and semi supervised learning, so that we can compare the performances of \n",
    "# both approaches \n",
    "df_train, df_test = train_test_split(df, test_size=0.3, random_state=42, stratify=df.issue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Preparing data\n",
    "df_train = cleaning_data(df_train)\n",
    "df_test = cleaning_data(df_test)\n",
    "dfu = cleaning_data(dfu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17547\n"
     ]
    }
   ],
   "source": [
    "## in order to have the same features on train data sets (for both supervised and semi-sup) and test data sets\n",
    "# build the tf idf with vocab which is the union the 3 above data sets \n",
    "vocab = list(set(itertools.chain(*dfu.stemmed_text.tolist()))|set(itertools.chain(*df_test.stemmed_text.tolist()))|set(itertools.chain(*df_train.stemmed_text.tolist())))\n",
    "vocab_dict = dict((y, x) for x, y in enumerate(vocab))\n",
    "print(len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# build tf idf matrix separately for train and test and unlabeled data sets \n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=2, ngram_range=(1,3), use_idf=True, vocabulary = vocab_dict)\n",
    "td_train = tfidf_vectorizer.fit_transform(df_train.joined_stemmed_text.tolist())\n",
    "td_test = tfidf_vectorizer.transform(df_test.joined_stemmed_text.tolist())\n",
    "td_u = tfidf_vectorizer.transform(dfu.joined_stemmed_text.tolist())\n",
    "#td_test = tfidf_vectorizer.fit_transform(df_test.joined_stemmed_text.tolist())\n",
    "#td_u = tfidf_vectorizer.fit_transform(dfu.joined_stemmed_text.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# same with NMF dimensionality reduction \n",
    "# the NMF decomposes this Term Document matrix into the product of 2 smaller matrices: W and H\n",
    "n_dimensions = 50 # This can also be interpreted as topics in this case. This is the \"beauty\" of NMF. 10 is arbitrary\n",
    "nmf_model = NMF(n_components=n_dimensions, random_state=42, alpha=.1, l1_ratio=.5)\n",
    "\n",
    "X_train = pd.DataFrame(nmf_model.fit_transform(td_train))\n",
    "X_test = pd.DataFrame(nmf_model.transform(td_test))\n",
    "X_u = pd.DataFrame(nmf_model.transform(td_u))\n",
    "#X_test = pd.DataFrame(nmf_model.fit_transform(td_test))\n",
    "#X_u = pd.DataFrame(nmf_model.fit_transform(td_u))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_train = df_train.issue.map(int)\n",
    "y_test = df_test.issue.map(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       0\n",
       "1       0\n",
       "2       0\n",
       "3       0\n",
       "4       0\n",
       "5       0\n",
       "6       0\n",
       "7       0\n",
       "8       0\n",
       "9       0\n",
       "10      0\n",
       "11      0\n",
       "12      0\n",
       "13      0\n",
       "14      0\n",
       "15      0\n",
       "16      0\n",
       "17      0\n",
       "18      0\n",
       "19      0\n",
       "20      0\n",
       "21      0\n",
       "22      0\n",
       "23      0\n",
       "24      0\n",
       "25      0\n",
       "26      0\n",
       "27      0\n",
       "28      0\n",
       "29      0\n",
       "       ..\n",
       "7664    0\n",
       "7665    0\n",
       "7666    0\n",
       "7667    0\n",
       "7668    0\n",
       "7669    0\n",
       "7670    0\n",
       "7671    0\n",
       "7672    1\n",
       "7673    0\n",
       "7674    0\n",
       "7675    0\n",
       "7676    0\n",
       "7677    0\n",
       "7678    0\n",
       "7679    0\n",
       "7680    0\n",
       "7681    0\n",
       "7682    0\n",
       "7683    0\n",
       "7684    0\n",
       "7685    0\n",
       "7686    0\n",
       "7687    0\n",
       "7688    0\n",
       "7689    0\n",
       "7690    0\n",
       "7691    0\n",
       "7692    0\n",
       "7693    0\n",
       "Name: screen, Length: 7658, dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7658\n",
      "7658\n",
      "1047\n",
      "449\n"
     ]
    }
   ],
   "source": [
    "# lets look at the number of positive in the data sets \n",
    "print(len(X_train))\n",
    "print(len(y_train))\n",
    "print(sum(y_train))\n",
    "print(sum(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# lets estimate a gradient boosting classifier \n",
    "model = GradientBoostingClassifier(n_estimators=100, random_state=42, learning_rate=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "              learning_rate=0.1, loss='deviance', max_depth=3,\n",
       "              max_features=None, max_leaf_nodes=None,\n",
       "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "              min_samples_leaf=1, min_samples_split=2,\n",
       "              min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "              presort='auto', random_state=42, subsample=1.0, verbose=0,\n",
       "              warm_start=False)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>apps_update</th>\n",
       "      <th>battery_life_charging</th>\n",
       "      <th>battery_overheat</th>\n",
       "      <th>camera</th>\n",
       "      <th>connectivity</th>\n",
       "      <th>customerservice</th>\n",
       "      <th>locking_system</th>\n",
       "      <th>memory_storage</th>\n",
       "      <th>screen</th>\n",
       "      <th>software_bugs</th>\n",
       "      <th>sound</th>\n",
       "      <th>system</th>\n",
       "      <th>water_damage</th>\n",
       "      <th>issue</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>finally, i really hated that apple didnÃ¢â¬â...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>so many crashes and battery drains,...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>it crashes for nothing and strikes with someth...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>only drawback is that it's very difficult for ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>slower than my windows phone from 2013.</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 text  apps_update  \\\n",
       "1   finally, i really hated that apple didnÃ¢â¬â...          0.0   \n",
       "4              so many crashes and battery drains,...          0.0   \n",
       "5   it crashes for nothing and strikes with someth...          0.0   \n",
       "19  only drawback is that it's very difficult for ...          0.0   \n",
       "26            slower than my windows phone from 2013.          0.0   \n",
       "\n",
       "    battery_life_charging  battery_overheat  camera  connectivity  \\\n",
       "1                     1.0               0.0     0.0           0.0   \n",
       "4                     1.0               0.0     0.0           0.0   \n",
       "5                     0.0               0.0     0.0           0.0   \n",
       "19                    0.0               0.0     0.0           0.0   \n",
       "26                    0.0               0.0     0.0           0.0   \n",
       "\n",
       "    customerservice  locking_system  memory_storage  screen  software_bugs  \\\n",
       "1               0.0             0.0             0.0     0.0            0.0   \n",
       "4               0.0             0.0             0.0     0.0            1.0   \n",
       "5               0.0             0.0             0.0     1.0            1.0   \n",
       "19              0.0             0.0             0.0     1.0            0.0   \n",
       "26              0.0             0.0             0.0     0.0            0.0   \n",
       "\n",
       "    sound  system  water_damage  issue  \n",
       "1     0.0     0.0           0.0    1.0  \n",
       "4     0.0     0.0           0.0    1.0  \n",
       "5     0.0     0.0           0.0    1.0  \n",
       "19    0.0     0.0           0.0    1.0  \n",
       "26    0.0     1.0           0.0    1.0  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[df.issue==1].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[6605    6]\n",
      " [ 953   94]]\n",
      "[[2819   16]\n",
      " [ 437   12]]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(y_train, model.predict(X_train)))\n",
    "print(confusion_matrix(y_test, model.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see that only 12 comments are labeled as \"issue\" by our prediction model on the testing set. And 437 that should have been detected did not get detected! This is pretty pretty bad. The reason might be that our Gradient Boosting method focuses on optimizing the prediction error, which is not the metric that makes sense in our case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.87      0.99      0.93      2835\n",
      "          1       0.43      0.03      0.05       449\n",
      "\n",
      "avg / total       0.81      0.86      0.81      3284\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, model.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### semi supervised learning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LabelPropagation(alpha=None, gamma=20, kernel='knn', max_iter=3000, n_jobs=1,\n",
       "         n_neighbors=10, tol=0.001)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.semi_supervised import LabelPropagation\n",
    "label_prop_model = LabelPropagation(kernel = 'knn', n_neighbors=10, max_iter = 3000)\n",
    "label_prop_model.fit(X_train, y_train)\n",
    "#label_prop_model.fit(pd.concat([X_train, X_test]), pd.concat([y_train, y_test]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What distance is used here? Because we are using a TF-IDF Matrix... Euclidian distance does not make sense.\n",
    "Here we are actually using it on the NMF. So the number of dimension is way lower."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    83783\n",
      "1      135\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "y_semi_proba = label_prop_model.predict_proba(X_u) # first column gives the proba of 0, second column gives the proba of 1 \n",
    "y_semi = pd.Series(label_prop_model.predict(X_u))\n",
    "print(y_semi.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    83918.000000\n",
       "mean         0.126843\n",
       "std          0.145547\n",
       "min          0.000000\n",
       "25%          0.000000\n",
       "50%          0.100000\n",
       "75%          0.200000\n",
       "max          1.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "proba_1 = y_semi_proba[:,1] # get the proba of 1 \n",
    "pd.Series(proba_1).describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see that with a 50% threshold it's maybe too strict for this case... Maybe we should lower this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[88827   226]\n",
      " [ 1550   973]]\n",
      "[[2793   42]\n",
      " [ 436   13]]\n"
     ]
    }
   ],
   "source": [
    "# with n neigh = 10\n",
    "X_train_semi = pd.concat([X_train, X_u])\n",
    "y_train_semi = pd.concat([y_train, y_semi])\n",
    "model.fit(X_train_semi, y_train_semi)\n",
    "\n",
    "print(confusion_matrix(y_train_semi, model.predict(X_train_semi)))\n",
    "print(confusion_matrix(y_test, model.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.86      0.99      0.92      2835\n",
      "          1       0.24      0.03      0.05       449\n",
      "\n",
      "avg / total       0.78      0.85      0.80      3284\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, model.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[82674   759]\n",
      " [ 5711  2432]]\n",
      "[[2768   67]\n",
      " [ 424   25]]\n"
     ]
    }
   ],
   "source": [
    "# try to spread more labels (use thereshold lower than 0.5 in order to predict more labels)\n",
    "# here we spread the same proportion of 1 in the unlabeled data set as in the labeled train data set  \n",
    "y_semi_bis = pd.Series([1 if x > pd.Series(proba_1).quantile(q=1-np.mean(y_train)) else 0 for x in proba_1])\n",
    "y_train_semi_bis = pd.concat([y_train, y_semi_bis])\n",
    "model.fit(X_train_semi, y_train_semi_bis)\n",
    "print(confusion_matrix(y_train_semi_bis, model.predict(X_train_semi)))\n",
    "print(confusion_matrix(y_test, model.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.87      0.98      0.92      2835\n",
      "          1       0.27      0.06      0.09       449\n",
      "\n",
      "avg / total       0.79      0.85      0.81      3284\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, model.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "##### We see that here the Label Propagation does not really improve our model... (or a bit only)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's try XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# lets estimate a XG boosting classifier \n",
    "XGmodel = xgb.XGBClassifier(n_estimators=100, random_state=42, learning_rate=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bytree=1, gamma=0, learning_rate=0.1, max_delta_step=0,\n",
       "       max_depth=3, min_child_weight=1, missing=None, n_estimators=100,\n",
       "       n_jobs=1, nthread=None, objective='binary:logistic',\n",
       "       random_state=42, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
       "       seed=None, silent=True, subsample=1)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "XGmodel.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[6603    8]\n",
      " [ 998   49]]\n",
      "[[2827    8]\n",
      " [ 443    6]]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(y_train, XGmodel.predict(X_train)))\n",
    "print(confusion_matrix(y_test, XGmodel.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see that only one comment was label as \"issue\" by our prediction model on the testing set. And 431 that should have been detected did not get detected! This is pretty pretty bad. The reason might be that our Gradient Boosting method focuses on optimizing the prediction error, which is not the metric that makes sense in our case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.86      1.00      0.93      2835\n",
      "          1       0.43      0.01      0.03       449\n",
      "\n",
      "avg / total       0.80      0.86      0.80      3284\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, XGmodel.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### semi supervised learning combined to XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[88854   199]\n",
      " [ 1659   864]]\n",
      "[[2816   19]\n",
      " [ 441    8]]\n"
     ]
    }
   ],
   "source": [
    "# with n neigh = 10\n",
    "XGmodel.fit(X_train_semi, y_train_semi)\n",
    "\n",
    "print(confusion_matrix(y_train_semi, XGmodel.predict(X_train_semi)))\n",
    "print(confusion_matrix(y_test, XGmodel.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.86      0.99      0.92      2835\n",
      "          1       0.30      0.02      0.03       449\n",
      "\n",
      "avg / total       0.79      0.86      0.80      3284\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, XGmodel.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[82833   600]\n",
      " [ 5936  2207]]\n",
      "[[2777   58]\n",
      " [ 425   24]]\n"
     ]
    }
   ],
   "source": [
    "# try to spread more labels (use thereshold lower than 0.5 in order to predict more labels)\n",
    "# here we spread the same proportion of 1 in the unlabeled data set as in the labeled train data set  \n",
    "y_semi_bis = pd.Series([1 if x > pd.Series(proba_1).quantile(q=1-np.mean(y_train)) else 0 for x in proba_1])\n",
    "y_train_semi_bis = pd.concat([y_train, y_semi_bis])\n",
    "XGmodel.fit(X_train_semi, y_train_semi_bis)\n",
    "print(confusion_matrix(y_train_semi_bis, XGmodel.predict(X_train_semi)))\n",
    "print(confusion_matrix(y_test, XGmodel.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.87      0.98      0.92      2848\n",
      "          1       0.27      0.04      0.08       432\n",
      "\n",
      "avg / total       0.79      0.86      0.81      3280\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, XGmodel.predict(X_test)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
