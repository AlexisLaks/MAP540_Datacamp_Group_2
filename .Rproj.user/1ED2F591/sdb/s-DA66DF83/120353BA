{
    "collab_server" : "",
    "contents" : "---\ntitle: \"MAP 540 Datacamp Capgemini: Group 2\"\nauthor: \"Sebastien Moeller\"\ndate: \"08/01/2018\"\noutput: html_document\n---\n\n# MAP540 Datacamp Project: Group 2\n## Dependencies\n```{r}\nif (!require(\"pacman\")) install.packages(\"pacman\")\n\npacman::p_load(RCurl, XML, dplyr, stringr, rvest, audio, sentimentr, lexicon)\n```\n\n## Functions\n### trim function removes unnecessary whitespace from html documents\n```{r}\ntrim <- function(x){\n  gsub(\"^\\\\s+|\\\\s+$\", \"\", x)\n}\n```\n\n### maxPageAmazon returns the number of pages of reviews a product has\n```{r}\nmaxPageAmazon <- function(code, region){\n  url <- paste0(\"http://www.amazon.\", region, \"/product-reviews/\",code)\n  \n  doc <- tryCatch(read_html(url), error = function(e){'empty page'})\n  if(doc[1] == 'empty page')\n    {\n    maxPage = -1\n    }\n  else\n    {\n    doc <- doc %>% html_nodes(\"a\") %>% html_text()\n    \n    idx = match(1, doc)\n    \n    if(is.na(idx))\n      {\n      maxPage = 1\n      }\n    else\n      {\n      maxPage = as.numeric(doc[match(1, doc)+4])\n      }\n    }\n  maxPage\n}\n```\n\n### amazonScraper takes the information from one page and turns it into a data frame\n```{r}\namazonScraper <- function(doc, reviewer = T, delay = 0){\n  \n  sec = 0\n  if(delay < 0) warning(\"delay was less than 0: set to 0\")\n  # Randomize the delay\n  if(delay > 0) sec = max(0, delay + runif(1, -1, 1))\n  \n  # Save different parts of the html document to the corresponding variable\n  title <- doc %>%\n    html_nodes(\"#cm_cr-review_list .a-color-base\") %>%\n    html_text()\n  \n  author <- doc %>%\n    html_nodes(\".review-byline .author\") %>%\n    html_text()\n  \n  date <- doc %>%\n    html_nodes(\"#cm_cr-review_list .review-date\") %>%\n    html_text() %>% \n    gsub(\".*on \", \"\", .)\n  \n  ver.purchase <- doc%>%\n    html_nodes(\".review-data.a-spacing-mini\") %>%\n    html_text() %>%\n    grepl(\"Verified Purchase\", .) %>%\n    as.numeric()\n\n  format <- doc %>% \n    html_nodes(\".review-data.a-spacing-mini\") %>% \n    html_text() %>%\n    gsub(\"Color: |\\\\|.*|Verified.*\", \"\", .)\n  #if(length(format) == 0) format <- NA\n  \n  stars <- doc %>%\n    html_nodes(\"#cm_cr-review_list  .review-rating\") %>%\n    html_text() %>%\n    str_extract(\"\\\\d\") %>%\n    as.numeric()\n  \n  comments <- doc %>%\n    html_nodes(\"#cm_cr-review_list .review-text\") %>%\n    html_text() \n  \n  helpful <- doc %>%\n    html_nodes(\".cr-vote-buttons .a-color-secondary\") %>%\n    html_text() %>%\n    str_extract(\"[:digit:]+|One\") %>%\n    gsub(\"One\", \"1\", .) %>%\n    as.numeric()\n  \n  if(reviewer == T){\n    rver_url <- doc %>%\n      html_nodes(\".review-byline .author\") %>%\n      html_attr(\"href\") %>%\n      gsub(\"/ref=cm_cr_othr_d_pdp\\\\?ie=UTF8\", \"\", .) %>%\n      gsub(\"/gp/pdp/profile/\", \"\", .) %>%\n      paste0(\"https://www.amazon.com/gp/cdp/member-reviews/\",.) \n    \n    #average rating of past 10 reviews\n    rver_avgrating_10 <- rver_url %>%\n      sapply(., function(x) {\n          read_html(x) %>%\n          html_nodes(\".small span img\") %>%\n          html_attr(\"title\") %>%\n          gsub(\"out of.*|stars\", \"\", .) %>%\n          as.numeric() %>%\n          mean(na.rm = T)\n      }) %>% as.numeric()\n  \n    rver_prof <- rver_url %>%\n      sapply(., function(x) \n        read_html(x) %>%\n          html_nodes(\"div.small, td td td .tiny\") %>%\n          html_text()\n      )\n    \n    rver_numrev <- rver_prof %>%\n      lapply(., function(x)\n        gsub(\"\\n  Customer Reviews: |\\n\", \"\", x[1])\n      ) %>% as.numeric()\n    \n    rver_numhelpful <- rver_prof %>%\n      lapply(., function(x)\n        gsub(\".*Helpful Votes:|\\n\", \"\", x[2]) %>%\n          trim()\n      ) %>% as.numeric()\n    \n    rver_rank <- rver_prof %>%\n      lapply(., function(x)\n        gsub(\".*Top Reviewer Ranking:|Helpful Votes:.*|\\n\", \"\", x[2]) %>%\n          removePunctuation() %>%\n          trim()\n      ) %>% as.numeric()\n    \n    df <- data.frame(title, date, ver.purchase, format, stars, comments, helpful,\n                     rver_url, rver_avgrating_10, rver_numrev, rver_numhelpful, rver_rank, stringsAsFactors = F)\n  \n  } else df <- data.frame(title, author, date, ver.purchase, format, stars, comments, helpful, stringsAsFactors = F)\n  \n  return(df)\n}\n```\n\n## Find relevant pages\nWe need a function that returns several amazon URLs of the product we are interested in \n```{r}\n\n```\n\n## Scraping function\nCalls amazonScraper function for each page of reviews\ninput: url, output: product reviews\n```{r}\nretrieveReviews <- function(url, delay = 2)\n  {\n  prod_code <- gsub(\"/.*\",\"\",gsub(\".*/dp/\", \"\", url))\n  prod_code\n  \n  region <- gsub(\"/.*\",\"\",gsub(\".*amazon.\", \"\", url))\n  region\n  \n  reviews_all <- NULL\n  \n  url <- paste0(\"https://www.amazon.\", region, \"/dp/\", prod_code)\n  if(tryCatch(read_html(url), error = function(e){'empty page'})[1] == 'empty page')\n    {\n    print(\"Please check URL\")\n    } \n  else\n    {\n    doc <- read_html(url)\n\n    #obtain the text in the node, remove \"\\n\" from the text, and remove white space\n    prod <- html_nodes(doc, \"#productTitle\") %>% html_text() %>% gsub(\"\\n\", \"\", .) %>% trim()\n    prod\n    \n    url <- paste0(\"https://www.amazon.\", region, \"/product-reviews/\", prod_code)\n    if(tryCatch(read_html(url), error = function(e){'empty page'})[1] == 'empty page')\n      {\n      # There are no reviews\n      } \n    else\n      {\n      numPage <- maxPageAmazon(prod_code, region)\n      if(numPage == -1)\n        {\n        # There are no reviews\n        } \n      else\n        {\n        for(page_num in 1:numPage)\n          {\n        \n          url <- paste0(\"http://www.amazon.\", region, \"/product-reviews/\", prod_code, \"/?pageNumber=\", page_num)\n          doc <- read_html(url)\n    \n          reviews <- amazonScraper(doc, reviewer = F, delay)\n        \n          if(numPage == 1)\n            {\n              reviews_all <- cbind(prod, reviews)\n            }\n          else\n            {\n              reviews_all <- rbind(reviews_all, cbind(prod, reviews))\n            }\n          }\n        }\n      }\n    }\n  reviews_all\n  }\n```\n\n# Start scraping\nOutput is a dataframe with all reviews and metrics included on the website\n```{r}\n# This example has only 1 page of reviews\nurl <- \"https://www.amazon.fr/Apple-iPhone-Smartphone-d%C3%A9bloqu%C3%A9-Ecran/dp/B075LYDD7Z/ref=sr_1_2?s=electronics&ie=UTF8&qid=1516045160&sr=1-2&keywords=iphone\"\n\n# This example has no reviews\n#url <- \"https://www.amazon.fr/Panpan-Protection-Anti-rayures-Ultra-claire-Transparent/dp/B0779Q1FKS/ref=sr_1_25_sspa?s=electronics&ie=UTF8&qid=1516043375&sr=1-25-spons&keywords=iphone+x&psc=1\"\n\n# This example has multiple pages of reviews\n#url <- \"https://www.amazon.fr/Ubegood-Anti-rayures-Absorption-Protection-Transparent/dp/B075FJWJYM/ref=sr_1_3?s=electronics&ie=UTF8&qid=1516049756&sr=1-3&keywords=iphone+x\"\n\ntest <- NULL\ntest <- retrieveReviews(url)\n#View(test)\n```\n\n\n\n\n\n\n\n\n# Sentiment analysis\n```{r}\nsent_agg <- with(test, sentiment_by(comments))\nhead(sent_agg)\n```\n\n```{r}\npar(mfrow=c(1,2))\nwith(test, hist(stars))\nwith(sent_agg, hist(ave_sentiment))\n```\n\n```{r}\nworst_reviews <- slice(test, top_n(sent_agg, 10, -ave_sentiment)$element_id)\nwith(worst_reviews, sentiment_by(comments)) %>% highlight()\n```\n\n```{r}\nbest_reviews <- slice(test, top_n(sent_agg, 10, ave_sentiment)$element_id)\nwith(best_reviews, sentiment_by(comments)) %>% highlight()\n```\n\n\n\n\n\n\n\n\n\n",
    "created" : 1516091667331.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "1795315271",
    "id" : "120353BA",
    "lastKnownWriteTime" : 1516091782,
    "last_content_update" : 1516091782209,
    "path" : "~/Capgemini/MAP540_Datacamp_Group_2/MAP540_Project.Rmd",
    "project_path" : "MAP540_Project.Rmd",
    "properties" : {
    },
    "relative_order" : 1,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_markdown"
}